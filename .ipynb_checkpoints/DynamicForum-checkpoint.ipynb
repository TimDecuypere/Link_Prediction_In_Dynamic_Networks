{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# node2vec\n",
    "---\n",
    "[node2vec](http://snap.stanford.edu/node2vec/) for link prediction:\n",
    "1. Perform train-test split\n",
    "1. Train skip-gram model on random walks within training graph\n",
    "2. Get node embeddings from skip-gram model\n",
    "3. Create bootstrapped edge embeddings by taking the Hadamard product of node embeddings\n",
    "4. Train a logistic regression classifier on these edge embeddings (possible edge --> edge score between 0-1)\n",
    "5. Evaluate these edge embeddings on the validation and test edge sets\n",
    "\n",
    "node2vec source code: https://github.com/aditya-grover/node2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: The Facebook-like Forum Network was attained from the same online community as the online social network; however, the focus in this network is not on the private messages exchanged among users, but on usersâ€™ activity in the forum.\n",
    "\n",
    "Number of nodes: 899\n",
    "Number of edges: 7046\n",
    "Number of time frames: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EGO_USER = 0 # which ego network to look at\n",
    "\n",
    "# Load pickled (adj, feat) tuple\n",
    "#network_dir = './fb-processed/{0}-adj-feat.pkl'.format(EGO_USER)\n",
    "#with open(network_dir, 'rb') as f:\n",
    "#    adj, features = pickle.load(f)\n",
    "    \n",
    "#g = nx.Graph(adj) # re-create graph using node indices (0 to num_nodes-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load in the networks, make the training and test edge lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899\n",
      "7046\n"
     ]
    }
   ],
   "source": [
    "MasterGraph = nx.read_edgelist(\"m_forum.csv\", nodetype=int, delimiter=\",\")\n",
    "for edge in MasterGraph.edges():\n",
    "    MasterGraph[edge[0]][edge[1]]['weight'] = 1\n",
    "\n",
    "print MasterGraph.number_of_nodes()\n",
    "print MasterGraph.number_of_edges()\n",
    "\n",
    "G1 = nx.read_edgelist(\"m1_forum.csv\", nodetype = int, delimiter = \",\")\n",
    "for edge in G1.edges():\n",
    "    G1[edge[0]][edge[1]]['weight'] = 1\n",
    "G2 = nx.read_edgelist(\"m2_forum.csv\", nodetype = int, delimiter = \",\")\n",
    "for edge in G2.edges():\n",
    "    G2[edge[0]][edge[1]]['weight'] = 1\n",
    "G3 = nx.read_edgelist(\"m3_forum.csv\", nodetype = int, delimiter = \",\")\n",
    "for edge in G3.edges():\n",
    "    G3[edge[0]][edge[1]]['weight'] = 1\n",
    "G4 = nx.read_edgelist(\"m4_forum.csv\", nodetype = int, delimiter = \",\")\n",
    "for edge in G4.edges():\n",
    "    G4[edge[0]][edge[1]]['weight'] = 1\n",
    "G13 = nx.read_edgelist(\"m4_forum.csv\", nodetype = int, delimiter = \",\")    \n",
    "for edge in G13.edges():\n",
    "    G13[edge[0]][edge[1]]['weight'] = 1\n",
    "\n",
    "## All the nodes are in MasterNodes    \n",
    "MasterNodes = MasterGraph.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training - Test split  \n",
    "'''''\n",
    "first add all the nodes that are in MasterGraph but not in \n",
    "G4\n",
    "'''''\n",
    "for i in MasterNodes:\n",
    "    if i not in G4.nodes():\n",
    "        G4.add_node(i)\n",
    "        \n",
    "adj_sparse = nx.to_scipy_sparse_matrix(G4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899\n",
      "2203\n",
      "3249\n",
      "3088\n",
      "3032\n"
     ]
    }
   ],
   "source": [
    "print G4.number_of_nodes()\n",
    "print G4.number_of_edges()\n",
    "print G1.number_of_edges()\n",
    "print G2.number_of_edges()\n",
    "print G3.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gae.preprocessing import mask_test_edges\n",
    "np.random.seed(0) # make sure train-test split is consistent between notebooks\n",
    "\n",
    "adj_sparse = nx.to_scipy_sparse_matrix(G4)\n",
    "\n",
    "adj_train, train_edges, train_edges_false, val_edges, val_edges_false, \\\n",
    "    test_edges, test_edges_false = mask_test_edges(adj_sparse, test_frac=.3, val_frac=.0, prevent_disconnect = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 899\n",
      "Total edges: 2202\n",
      "Training edges (positive): 1541\n",
      "Training edges (negative): 1541\n",
      "Validation edges (positive): 0\n",
      "Validation edges (negative): 0\n",
      "Test edges (positive): 660\n",
      "Test edges (negative): 660\n"
     ]
    }
   ],
   "source": [
    "# Inspect train/test split\n",
    "print \"Total nodes:\", adj_sparse.shape[0]\n",
    "print \"Total edges:\", int(adj_sparse.nnz/2) # adj is symmetric, so nnz (num non-zero) = 2*num_edges\n",
    "print \"Training edges (positive):\", len(train_edges)\n",
    "print \"Training edges (negative):\", len(train_edges_false)\n",
    "print \"Validation edges (positive):\", len(val_edges)\n",
    "print \"Validation edges (negative):\", len(val_edges_false)\n",
    "print \"Test edges (positive):\", len(test_edges)\n",
    "print \"Test edges (negative):\", len(test_edges_false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive training edges are in the edgelist \"train_edges\".\n",
    "\n",
    "The negative training edges are in the edgelist \"train_edges_false\".\n",
    "\n",
    "The positive test edges are in the edgelist \"test_edges\".\n",
    "\n",
    "The negative test edges are in the edgelist \"test_edges_false\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: add all the nodes in G1, G2, G3 that are not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "add all the nodes that are in the MasterGraph but not in \n",
    "G1, G2 and G3\n",
    "'''\n",
    "for i in MasterNodes:\n",
    "    if i not in G1.nodes():\n",
    "        G1.add_node(i)\n",
    "    if i not in G2.nodes():\n",
    "        G2.add_node(i)\n",
    "    if i not in G3.nodes():\n",
    "        G3.add_node(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges before removal: \n",
      "G1:   3249\n",
      "G2:   3088\n",
      "G3:   3032\n",
      "Edges after removal: \n",
      "G1:   3237\n",
      "G2:   3078\n",
      "G3:   3021\n"
     ]
    }
   ],
   "source": [
    "print \"Edges before removal: \"\n",
    "print \"G1:  \", G1.number_of_edges()\n",
    "print \"G2:  \", G2.number_of_edges()\n",
    "print \"G3:  \", G3.number_of_edges()\n",
    "\n",
    "\n",
    "'''\n",
    "for every snapshot, delete all the edges that occur in the \n",
    "test set, this is important because the training of node2vec\n",
    "can only be done on the training network and not on edges that\n",
    "are used for testing\n",
    "'''\n",
    "for i in range(0,len(test_edges)):\n",
    "        if G1.has_edge(test_edges[i, 0], test_edges[i, 1]):\n",
    "            G1.remove_edge(test_edges[i, 0], test_edges[i, 1])\n",
    "        if G2.has_edge(test_edges[i, 0], test_edges[i, 1]):\n",
    "            G2.remove_edge(test_edges[i, 0], test_edges[i, 1])\n",
    "        if G3.has_edge(test_edges[i, 0], test_edges[i, 1]):\n",
    "            G3.remove_edge(test_edges[i, 0], test_edges[i, 1])\n",
    "            \n",
    "print \"Edges after removal: \"\n",
    "print \"G1:  \", G1.number_of_edges()\n",
    "print \"G2:  \", G2.number_of_edges()\n",
    "print \"G3:  \", G3.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train node2vec (Learn Node Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim_Dell_XPS\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import node2vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node2vec settings\n",
    "# NOTE: When p = q = 1, this is equivalent to DeepWalk\n",
    "\n",
    "P = 1 # Return hyperparameter\n",
    "Q = 1 # In-out hyperparameter\n",
    "WINDOW_SIZE = 10 # Context size for optimization\n",
    "NUM_WALKS = 10 # Number of walks per source\n",
    "WALK_LENGTH = 80 # Length of walk per source\n",
    "DIMENSIONS = 128 # Embedding dimension\n",
    "DIRECTED = False # Graph directed/undirected\n",
    "WORKERS = 8 # Num. parallel workers\n",
    "ITER = 1 # SGD epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n",
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n",
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing, generate walks\n",
    "G1_n2v = node2vec.Graph(G1, DIRECTED, P, Q) # create node2vec graph instance\n",
    "G2_n2v = node2vec.Graph(G2, DIRECTED, P, Q)\n",
    "G3_n2v = node2vec.Graph(G3, DIRECTED, P, Q)\n",
    "\n",
    "G1_n2v.preprocess_transition_probs()\n",
    "G2_n2v.preprocess_transition_probs()\n",
    "G3_n2v.preprocess_transition_probs()\n",
    "\n",
    "walksG1 = G1_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH)\n",
    "walksG2 = G2_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH)\n",
    "walksG3 = G3_n2v.simulate_walks(NUM_WALKS, WALK_LENGTH)\n",
    "\n",
    "walksG1 = [map(str, walk) for walk in walksG1]\n",
    "walksG2 = [map(str, walk) for walk in walksG2]\n",
    "walksG3 = [map(str, walk) for walk in walksG3]\n",
    "\n",
    "# Train skip-gram model\n",
    "modelG1 = Word2Vec(walksG1, size=DIMENSIONS, window=WINDOW_SIZE, min_count=0, sg=1, workers=WORKERS, iter=ITER)\n",
    "modelG2 = Word2Vec(walksG2, size=DIMENSIONS, window=WINDOW_SIZE, min_count=0, sg=1, workers=WORKERS, iter=ITER)\n",
    "modelG3 = Word2Vec(walksG3, size=DIMENSIONS, window=WINDOW_SIZE, min_count=0, sg=1, workers=WORKERS, iter=ITER)\n",
    "\n",
    "\n",
    "# Store embeddings mapping\n",
    "emb_mappingsG1 = modelG1.wv\n",
    "emb_mappingsG2 = modelG2.wv\n",
    "emb_mappingsG3 = modelG3.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Edge Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create node embeddings matrix (rows = nodes, columns = embedding features)\n",
    "emb_listG1 = []\n",
    "emb_listG2 = []\n",
    "emb_listG3 = []\n",
    "\n",
    "for node_index in range(1, adj_sparse.shape[0]+1):\n",
    "    node_str = str(node_index)\n",
    "    \n",
    "    node_embG1 = emb_mappingsG1[node_str]\n",
    "    node_embG2 = emb_mappingsG2[node_str]\n",
    "    node_embG3 = emb_mappingsG3[node_str]\n",
    "    \n",
    "    emb_listG1.append(node_embG1)\n",
    "    emb_listG2.append(node_embG2)\n",
    "    emb_listG3.append(node_embG3)\n",
    "    \n",
    "emb_matrixG1 = np.vstack(emb_listG1)\n",
    "emb_matrixG2 = np.vstack(emb_listG2)\n",
    "emb_matrixG3 = np.vstack(emb_listG3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bootstrapped edge embeddings (as is done in node2vec paper)\n",
    "    # Edge embedding for (v1, v2) = hadamard product of node embeddings for v1, v2\n",
    "def get_edge_embeddings(edge_list):\n",
    "    embs = []\n",
    "    for edge in edge_list:\n",
    "        \n",
    "        node1 = edge[0]\n",
    "        node2 = edge[1]\n",
    "        \n",
    "        embG1_1 = emb_matrixG1[node1]\n",
    "        embG1_2 = emb_matrixG1[node2]\n",
    "        \n",
    "        embG2_1 = emb_matrixG2[node1]\n",
    "        embG2_2 = emb_matrixG2[node2]\n",
    "        \n",
    "        embG3_1 = emb_matrixG3[node1]\n",
    "        embG3_2 = emb_matrixG3[node2]\n",
    "        \n",
    "        edge_embG1 = np.multiply(embG1_1, embG1_2)\n",
    "        edge_embG2 = np.multiply(embG2_1, embG2_2)\n",
    "        edge_embG3 = np.multiply(embG3_1, embG3_2)\n",
    "        \n",
    "        edge_emb = np.hstack((edge_embG1,edge_embG2, edge_embG3))\n",
    "        embs.append(edge_emb)\n",
    "        \n",
    "    embs = np.array(embs)\n",
    "    \n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-set edge embeddings\n",
    "pos_train_edge_embs = get_edge_embeddings(train_edges)\n",
    "neg_train_edge_embs = get_edge_embeddings(train_edges_false)\n",
    "train_edge_embs = np.concatenate([pos_train_edge_embs, neg_train_edge_embs])\n",
    "\n",
    "# Create train-set edge labels: 1 = real edge, 0 = false edge\n",
    "train_edge_labels = np.concatenate([np.ones(len(train_edges)), np.zeros(len(train_edges_false))])\n",
    "\n",
    "# Val-set edge embeddings, labels\n",
    "pos_val_edge_embs = get_edge_embeddings(val_edges)\n",
    "neg_val_edge_embs = get_edge_embeddings(val_edges_false)\n",
    "val_edge_embs = np.concatenate([pos_val_edge_embs, neg_val_edge_embs])\n",
    "val_edge_labels = np.concatenate([np.ones(len(val_edges)), np.zeros(len(val_edges_false))])\n",
    "\n",
    "# Test-set edge embeddings, labels\n",
    "pos_test_edge_embs = get_edge_embeddings(test_edges)\n",
    "neg_test_edge_embs = get_edge_embeddings(test_edges_false)\n",
    "test_edge_embs = np.concatenate([pos_test_edge_embs, neg_test_edge_embs])\n",
    "\n",
    "# Create val-set edge labels: 1 = real edge, 0 = false edge\n",
    "test_edge_labels = np.concatenate([np.ones(len(test_edges)), np.zeros(len(test_edges_false))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Edge Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train logistic regression classifier on train-set edge embeddings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "edge_classifier = LogisticRegression(random_state=0)\n",
    "edge_classifier.fit(train_edge_embs, train_edge_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted edge scores: probability of being of class \"1\" (real edge)\n",
    "# val_preds = edge_classifier.predict_proba(val_edge_embs)[:, 1]\n",
    "# val_roc = roc_auc_score(val_edge_labels, val_preds)\n",
    "# val_ap = average_precision_score(val_edge_labels, val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted edge scores: probability of being of class \"1\" (real edge)\n",
    "test_preds = edge_classifier.predict_proba(test_edge_embs)[:, 1]\n",
    "test_roc = roc_auc_score(test_edge_labels, test_preds)\n",
    "test_ap = average_precision_score(test_edge_labels, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node2vec Test ROC score:  0.8206955922865014\n",
      "node2vec Test AP score:  0.8403788669295117\n"
     ]
    }
   ],
   "source": [
    "# print 'node2vec Validation ROC score: ', str(val_roc)\n",
    "# print 'node2vec Validation AP score: ', str(val_ap)\n",
    "print 'node2vec Test ROC score: ', str(test_roc)\n",
    "print 'node2vec Test AP score: ', str(test_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Baseline classifiers (Adamic Adar, Jaccard Index & Preferred Attachement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_score(edges_pos, edges_neg, score_matrix):\n",
    "    # Store positive edge predictions, actual values\n",
    "    preds_pos = []\n",
    "    pos = []\n",
    "    for edge in edges_pos:\n",
    "        preds_pos.append(score_matrix[edge[0], edge[1]]) # predicted score\n",
    "        pos.append(adj_sparse[edge[0], edge[1]]) # actual value (1 for positive)\n",
    "        \n",
    "    # Store negative edge predictions, actual values\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for edge in edges_neg:\n",
    "        preds_neg.append(score_matrix[edge[0], edge[1]]) # predicted score\n",
    "        neg.append(adj_sparse[edge[0], edge[1]]) # actual value (0 for negative)\n",
    "        \n",
    "    # Calculate scores\n",
    "    preds_all = np.hstack([preds_pos, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds_pos)), np.zeros(len(preds_neg))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "adj_sparse = nx.to_scipy_sparse_matrix(MasterGraph)    \n",
    "    \n",
    "adj_train, train_edges, train_edges_false, val_edges, val_edges_false, \\\n",
    "    test_edges, test_edges_false = mask_test_edges(adj_sparse, test_frac=.3, val_frac=.00, prevent_disconnect=False)    \n",
    "    \n",
    "g_train = nx.from_scipy_sparse_matrix(adj_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adamic-Adar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-4b07c00b8b9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Compute Adamic-Adar indexes from g_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0maa_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madamic_adar_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# (u, v) = node indices, p = Adamic-Adar index\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0maa_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0maa_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m \u001b[1;31m# make sure it's symmetric\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'adj' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute Adamic-Adar indexes from g_train\n",
    "aa_matrix = np.zeros(adj.shape)\n",
    "for u, v, p in nx.adamic_adar_index(g_train): # (u, v) = node indices, p = Adamic-Adar index\n",
    "    aa_matrix[u][v] = p\n",
    "    aa_matrix[v][u] = p # make sure it's symmetric\n",
    "    \n",
    "# Normalize array\n",
    "aa_matrix = aa_matrix / aa_matrix.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC AUC and Average Precision\n",
    "aa_roc, aa_ap = get_roc_score(test_edges, test_edges_false, aa_matrix)\n",
    "\n",
    "print 'Adamic-Adar Test ROC score: ', str(aa_roc)\n",
    "print 'Adamic-Adar Test AP score: ', str(aa_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Jaccard Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Jaccard Coefficients from g_train\n",
    "jc_matrix = np.zeros(adj.shape)\n",
    "for u, v, p in nx.jaccard_coefficient(g_train): # (u, v) = node indices, p = Jaccard coefficient\n",
    "    jc_matrix[u][v] = p\n",
    "    jc_matrix[v][u] = p # make sure it's symmetric\n",
    "    \n",
    "# Normalize array\n",
    "jc_matrix = jc_matrix / jc_matrix.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC AUC and Average Precision\n",
    "jc_roc, jc_ap = get_roc_score(test_edges, test_edges_false, jc_matrix)\n",
    "\n",
    "print 'Jaccard Coefficient Test ROC score: ', str(jc_roc)\n",
    "print 'Jaccard Coefficient Test AP score: ', str(jc_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate, store Adamic-Index scores in array\n",
    "pa_matrix = np.zeros(adj.shape)\n",
    "for u, v, p in nx.preferential_attachment(g_train): # (u, v) = node indices, p = Jaccard coefficient\n",
    "    pa_matrix[u][v] = p\n",
    "    pa_matrix[v][u] = p # make sure it's symmetric\n",
    "    \n",
    "# Normalize array\n",
    "pa_matrix = pa_matrix / pa_matrix.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC AUC and Average Precision\n",
    "pa_roc, pa_ap = get_roc_score(test_edges, test_edges_false, pa_matrix)\n",
    "\n",
    "print 'Preferential Attachment Test ROC score: ', str(pa_roc)\n",
    "print 'Preferential Attachment Test AP score: ', str(pa_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
